{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "HW-1 Part 1. Spam Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Z8eeN5IW9q"
      },
      "source": [
        "# Part 1.\n",
        "\n",
        "The deadline for Part 1 is **2 pm Feb 11, 2021**.   \n",
        "You should submit a `.ipynb` file with your solutions to NYU Classes.\n",
        "\n",
        "---\n",
        "Spam filtering is a well-studied NLP classification problem that is used in many commercial products nowadays, including email clients and mobile SMS apps.\n",
        "\n",
        "In this assignment we will train a logistic regression model to classify each text in the SMS Spam Collection dataset as either spam or legitimate. This dataset consists of 5,574 English SMS messages, each tagged as either spam or ham (legitimate). The column 'v1' contains the label and the column 'v2' contains the SMS text. We will pre-process and convert the texts into bag-of-words features for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZd0LJzbISPd"
      },
      "source": [
        "## Data Loading\n",
        "\n",
        "First, we download the SMS Spam Collection Dataset. The below command downloads the dataset from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loads it to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvGErs2oHkWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63dd4495-ecd9-4a6e-ab84-39f8bca49242"
      },
      "source": [
        "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-03 19:46:53--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.214.100, 173.194.214.113, 173.194.214.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.214.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lchte0302sib3tki49t8pf47pr3ck0c3/1612381575000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-02-03 19:46:53--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/lchte0302sib3tki49t8pf47pr3ck0c3/1612381575000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
            "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 173.194.217.132, 2607:f8b0:400c:c13::84\n",
            "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|173.194.217.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503663 (492K) [text/csv]\n",
            "Saving to: ‘spam.csv’\n",
            "\n",
            "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.006s  \n",
            "\n",
            "2021-02-03 19:46:54 (84.2 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcHV1lUwtH-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b1b348-57f8-4c17-f7e6-b47f2becb168"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spam.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXVQCF-ovo4G"
      },
      "source": [
        "Now we preview the data. There are two columns: `v1` -- the label, which indicates whether the text is spam or ham (legitimate), and `v2` -- the text of the message."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiKE89v0zMiY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d028a0c1-c82a-4371-d821-01cc6d3528ec"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
        "# 1 - spam, 0 - ham\n",
        "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   v1                                                 v2\n",
              "0   0  Go until jurong point, crazy.. Available only ...\n",
              "1   0                      Ok lar... Joking wif u oni...\n",
              "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   0  U dun say so early hor... U c already then say...\n",
              "4   0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXQhTzrCv-Nk"
      },
      "source": [
        "Your task is to randomly split the data into training, validation, and test sets. Make sure that each row appears in only one of the splits and that training contains 70% of the data, validation contains 15%, and test contains 15%. You can compute the size of each split afterwards to sanity check your code. **You may use numpy and pandas functions, but please do not use sklearn.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga5Qydpw-gdQ"
      },
      "source": [
        "# 0.15 for val, 0.15 for test, 0.7 for train\n",
        "val_size = int(df.shape[0] * 0.15)\n",
        "test_size = int(df.shape[0] * 0.15)\n",
        "\n",
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "\n",
        "train_texts, train_labels = None, None\n",
        "val_texts, val_labels     = None, None\n",
        "test_texts, test_labels   = None, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGyHG4lBISP2"
      },
      "source": [
        "## Data Processing\n",
        "\n",
        "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. In Lab 2 we will use the built-in `sklearn` function, `sklearn.feature_extraction.text.CountVectorizer`. \n",
        "**In this HW, you are required to implement the class `Vectorizer` on your own without using `sklearn` built-in functions.**\n",
        "\n",
        "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
        "You may use [spacy](https://spacy.io/) or [nltk](https://www.nltk.org/) text processing libraries in `preprocess_data` function. \n",
        "\n",
        "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "793EFaQYhHeR"
      },
      "source": [
        "def preprocess_data(data):\n",
        "    # This function should return a list of lists of pre-processed tokens, where\n",
        "    # each nested list contains the tokens for a single message. To pre-process\n",
        "    # each message, tokenize the message and lowercase all text.\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    preprocessed_data = None\n",
        "    return preprocessed_data\n",
        "\n",
        "train_data = preprocess_data(train_texts)\n",
        "val_data = preprocess_data(val_texts)\n",
        "test_data = preprocess_data(test_texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw8z2TjpS3Pv"
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM2qpOKpjVbD"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class Vectorizer():\n",
        "    def __init__(self, max_features):\n",
        "        self.max_features = max_features\n",
        "        self.vocab_list = None\n",
        "        self.token_to_index = None\n",
        "\n",
        "    def fit(self, dataset):\n",
        "        # Given a list of lists of tokens, create a vocab list, self.vocab_list, \n",
        "        # using the most frequent \"max_features\" tokens. Also create a token \n",
        "        # indexer, self.token_to_index, that will return the index of the token \n",
        "        # in self.vocab_list.\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def transform(self, dataset):\n",
        "        # This function transforms the text dataset (a list of lists of tokens) \n",
        "        # into a matrix, \"data_matrix,\" where the entry located at (i, j) is 1 \n",
        "        # if sample i of the dataset contains token j in the vocab list, and 0 \n",
        "        # otherwise.\n",
        "        \"\"\"\n",
        "        YOUR CODE GOES HERE\n",
        "        \"\"\"\n",
        "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
        "        \n",
        "        return data_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXMrZXlZjcH7"
      },
      "source": [
        "max_features = None # TODO: Replace None with a number\n",
        "vectorizer = Vectorizer(max_features=max_features)\n",
        "vectorizer.fit(train_data)\n",
        "X_train = vectorizer.transform(train_data)\n",
        "X_val = vectorizer.transform(val_data)\n",
        "X_test = vectorizer.transform(test_data)\n",
        "\n",
        "y_train = np.array(train_labels)\n",
        "y_val = np.array(val_labels)\n",
        "y_test = np.array(test_labels)\n",
        "\n",
        "vocab = vectorizer.vocab_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGLg6udky1zo"
      },
      "source": [
        "You can add more features to the feature matrix. (Not required, but worth extra credit points.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80GgEm6F5DG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b0b32c5-85a7-4a8a-df26-fe4749db7580"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYOUR CODE GOES HERE\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtm7a6JWu9-3"
      },
      "source": [
        "## Model\n",
        "\n",
        "We train a logistic regression model on the bag-of-words features and save predictions for the training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9stSAbAIZe"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(random_state=0, solver='liblinear')\n",
        "\n",
        "# Fit the model to training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction using the trained model\n",
        "y_train_pred = model.predict(X_train)\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_test_pred = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j-Abw7JOqD_"
      },
      "source": [
        "## Performance of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akg9LvP5DGE8"
      },
      "source": [
        "Your task is to report train, val, test accuracies and F1 scores.\n",
        "**You are required to implement `accuracy_score` and `f1_score` methods without using built-in python functions.**\n",
        "\n",
        "Your model should achieve at least **0.95** test accuracy and **0.90** test F1 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chqVbKH6kZyY"
      },
      "source": [
        "def accuracy_score(y_true, y_pred): \n",
        "    # Calculate accuracy of the model's prediction\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    accuracy = None\n",
        "    return accuracy\n",
        "\n",
        "def f1_score(y_true, y_pred): \n",
        "    # Calculate F1 score of the model's prediction\n",
        "    \"\"\"\n",
        "    YOUR CODE GOES HERE\n",
        "    \"\"\"\n",
        "    f1 = None\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqrMw0udDD04"
      },
      "source": [
        "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
        "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
        "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
        "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW7P84giGgP4"
      },
      "source": [
        "**Question.**\n",
        "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak0h71krLPqX"
      },
      "source": [
        "**Question.**\n",
        "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_RDI0qdOxwM"
      },
      "source": [
        "### Exploration of predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHR2OqYCDOxs"
      },
      "source": [
        "Show a few examples with correctly predicted labels on the train and val sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yv8GD-UGXvR"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\"\n",
        "# 1 - spam, 0 - ham\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neMQ4VR9GVL3"
      },
      "source": [
        "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ssK0jRxGY3u"
      },
      "source": [
        "\"\"\"\n",
        "YOUR CODE GOES HERE\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja1hoUIFp_C2"
      },
      "source": [
        "## End of Part 1.\n"
      ]
    }
  ]
}